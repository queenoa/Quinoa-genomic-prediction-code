{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff79881",
   "metadata": {},
   "source": [
    "# Machine Learning Approach using LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad0c647",
   "metadata": {},
   "source": [
    "## Prepare Model Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2c4345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "### load input data\n",
    "\n",
    "# phenotype data\n",
    "pheno = pd.read_csv('data/AUSPAK_phenotypes_means_BLUEs.csv', index_col=0)\n",
    "pheno = pheno.rename(columns={'SampleName': 'sample.id'})\n",
    "\n",
    "# load PCA data\n",
    "pca = pd.read_csv('quinoa_551_AUSPAK_PCs.csv')\n",
    "\n",
    "## merge and prepare model input data\n",
    "\n",
    "# combine PCA and phenotype data\n",
    "model_input = pd.merge(pca, pheno, on='sample.id')\n",
    "\n",
    "\n",
    "### ONE-HOT ENCODE CATEGORICAL VARIABLES\n",
    "\n",
    "# as binary features for ML models\n",
    "cat_encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_features = cat_encoder.fit_transform(model_input[['location', 'environment']])\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_features, \n",
    "    columns=cat_encoder.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# Combine original data with encoded features\n",
    "model_input_final = pd.concat([model_input, encoded_df], axis=1)\n",
    "\n",
    "# Remove year column (there is nothing linking year across locations, so it is not useful for prediction)\n",
    "model_input_final = model_input_final.drop(columns=['year'])\n",
    "\n",
    "\n",
    "### save prepared data\n",
    "\n",
    "model_input_final.to_pickle('model_inputs/model_input.pkl')\n",
    "\n",
    "print(f\"Model input prepared: {model_input_final.shape[0]} observations, {model_input_final.shape[1]} features\")\n",
    "print(f\"Saved to: model_inputs/model_input.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5fe0a6",
   "metadata": {},
   "source": [
    "## Machine Learning Genomic Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7977370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ndcg_score\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "### Evaluation metrics\n",
    "def pearson_corr(y_true, y_pred):\n",
    "    \"\"\"Calculate Pearson correlation between true and predicted values\"\"\"\n",
    "    return np.corrcoef(y_true, y_pred)[0, 1]\n",
    "\n",
    "def calculate_ndcg(y_true, y_pred, k=10, lower_is_better=False):\n",
    "    \"\"\"\n",
    "    Calculate NDCG@k for genomic prediction\n",
    "    Measures ranking quality by comparing predicted ranking to ideal ranking\n",
    "    \"\"\"\n",
    "    k = min(k, len(y_true)) # handle case where less than k samples\n",
    "    \n",
    "    # Flip values if lower is better\n",
    "    if lower_is_better:\n",
    "        y_true_adj = -y_true\n",
    "        y_pred_adj = -y_pred\n",
    "    else:\n",
    "        y_true_adj = y_true\n",
    "        y_pred_adj = y_pred\n",
    "    \n",
    "    # Transform to non-negative values for NDCG calculation\n",
    "    min_val = min(y_true_adj.min(), y_pred_adj.min())\n",
    "    if min_val < 0:\n",
    "        y_true_pos = y_true_adj - min_val + 1e-6 \n",
    "        y_pred_pos = y_pred_adj - min_val + 1e-6\n",
    "    else:\n",
    "        y_true_pos = y_true_adj\n",
    "        y_pred_pos = y_pred_adj\n",
    "    \n",
    "    # Reshape for sklearn (expects 2D)\n",
    "    y_true_2d = y_true_pos.reshape(1, -1)\n",
    "    y_pred_2d = y_pred_pos.reshape(1, -1)\n",
    "    \n",
    "    return ndcg_score(y_true_2d, y_pred_2d, k=k)\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred, trait_name=None):\n",
    "    \"\"\"Comprehensive evaluation including ranking metrics\"\"\"\n",
    "    if len(y_true) < 2:\n",
    "        return {'pearson': np.nan, 'ndcg_at_10': np.nan}\n",
    "    \n",
    "    # Define traits where lower values are better\n",
    "    lower_is_better_traits = ['DTF_blue', 'DTH_blue', 'PtHt_blue', 'DTF_mean', 'DTH_mean', 'PtHt_mean']\n",
    "    lower_is_better = trait_name in lower_is_better_traits if trait_name else False\n",
    "    \n",
    "    try:\n",
    "        return {\n",
    "            'pearson': pearson_corr(y_true, y_pred),\n",
    "            'ndcg_at_10': calculate_ndcg(y_true, y_pred, k=10, lower_is_better=lower_is_better)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"    Warning: Error calculating metrics: {str(e)}\")\n",
    "        return {'pearson': np.nan, 'ndcg_at_10': np.nan}\n",
    "\n",
    "### data preprocessing\n",
    "\n",
    "def apply_environment_scaling(data: pd.DataFrame, traits: list) -> pd.DataFrame:\n",
    "    \"\"\"Apply z-score transformation by environment to all traits upfront\"\"\"\n",
    "    data_scaled = data.copy()\n",
    "    \n",
    "    for trait in traits:\n",
    "        trait_data = data_scaled[data_scaled[trait].notna()].copy()\n",
    "        if len(trait_data) < 50:\n",
    "            continue\n",
    "            \n",
    "        print(f\"Applying z-score transformation by environment for {trait}...\")\n",
    "        \n",
    "        for env in trait_data['environment'].unique():\n",
    "            env_mask = (data_scaled['environment'] == env) & (data_scaled[trait].notna())\n",
    "            \n",
    "            if env_mask.sum() > 1:  # Need at least 2 points to calculate std\n",
    "                scaler = StandardScaler()\n",
    "                values = data_scaled.loc[env_mask, trait].values.reshape(-1, 1)\n",
    "                data_scaled.loc[env_mask, trait] = scaler.fit_transform(values).flatten()\n",
    "                print(f\"  Environment {env}: n={env_mask.sum()}\")\n",
    "    \n",
    "    return data_scaled\n",
    "\n",
    "### cross-validation\n",
    "\n",
    "def run_location_specific_cv(data: pd.DataFrame, traits: list, model_class=LGBMRegressor,\n",
    "                           model_params: dict = None, n_iterations: int = 15, \n",
    "                           k_folds: int = 5, n_jobs: int = 10, apply_zscore: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run grouped k-fold cross-validation for genomic prediction with location-specific evaluation\n",
    "\n",
    "    Strategy:\n",
    "    - Split genotypes (not observations) into folds to prevent data leakage\n",
    "    - Train on all observations from training genotypes across all environments\n",
    "    - Predict for test genotypes and evaluate separately by location\n",
    "    - Repeat with different random seeds for robust estimates\n",
    "    \"\"\"\n",
    "    \n",
    "    if model_params is None:\n",
    "        model_params = {'max_depth': 3, 'learning_rate': 0.05, 'n_estimators': 500}\n",
    "    \n",
    "    # Initialize results dataframe\n",
    "    cv_results = pd.DataFrame(columns=[\n",
    "        'iteration', 'fold', 'trait', 'location', 'pearson',\n",
    "        'ndcg_at_10', 'seed', 'n_observations', 'n_test_genotypes'\n",
    "    ])\n",
    "    \n",
    "    # Define feature columns (PCs + encoded location/env)\n",
    "    feature_columns = [col for col in data.columns if col.startswith('PC') or \n",
    "                      col.startswith('location_') or col.startswith('environment_')]\n",
    "    \n",
    "    # Get unique genotypes and locations\n",
    "    genotypes = data['sample.id'].unique()\n",
    "    locations = data['location'].unique()\n",
    "    \n",
    "    print(f\"Starting cross-validation with {len(genotypes)} genotypes\")\n",
    "    print(f\"Traits: {', '.join(traits)}\")\n",
    "    print(f\"Locations: {', '.join(locations)}\")\n",
    "    print(f\"Feature columns: {len(feature_columns)} (PCs + encoded variables)\")\n",
    "    print(f\"K-folds: {k_folds} | Iterations: {n_iterations}\")\n",
    "    print(f\"Z-score transformation: {'ENABLED' if apply_zscore else 'DISABLED'}\")\n",
    "    print(\"Evaluation metrics: Pearson, NDCG@10 (higher is better)\\n\")\n",
    "    \n",
    "    # Apply environment scaling upfront if enabled\n",
    "    if apply_zscore:\n",
    "        data = apply_environment_scaling(data, traits)\n",
    "\n",
    "    ### cross validation loop\n",
    "    # Loop through iterations with different seeds\n",
    "    for iter_num in range(1, n_iterations + 1):\n",
    "        current_seed = 1000 + iter_num\n",
    "        np.random.seed(current_seed)\n",
    "        \n",
    "        print(f\"Iteration {iter_num} of {n_iterations} (seed: {current_seed})\")\n",
    "        \n",
    "        # Create genotype folds (GroupKFold ensures genotypes are grouped)\n",
    "        gkf = GroupKFold(n_splits=k_folds)\n",
    "        fold_splits = list(gkf.split(data, groups=data['sample.id']))\n",
    "        \n",
    "        # Loop through folds\n",
    "        for fold_idx, (train_idx, test_idx) in enumerate(fold_splits, 1):\n",
    "            print(f\"  Fold {fold_idx} of {k_folds}\")\n",
    "            \n",
    "            # Get test genotypes for this fold\n",
    "            test_genotypes = data.iloc[test_idx]['sample.id'].unique()\n",
    "            \n",
    "            # Loop through traits\n",
    "            for trait in traits:\n",
    "                try:\n",
    "                    # Check if trait has enough data\n",
    "                    trait_data = data[data[trait].notna()].copy()\n",
    "                    if len(trait_data) < 50:\n",
    "                        print(f\"    Warning: Not enough data for trait {trait}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Create training data by masking test genotypes\n",
    "                    train_data = trait_data[~trait_data['sample.id'].isin(test_genotypes)].copy()\n",
    "                    \n",
    "                    # Check if we have enough training data\n",
    "                    if len(train_data) < 50:\n",
    "                        print(f\"    Warning: Not enough training data for trait {trait} (N={len(train_data)})\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Prepare features and target for training\n",
    "                    X_train = train_data[feature_columns]\n",
    "                    y_train = train_data[trait]\n",
    "                    \n",
    "                    # train model\n",
    "                    model = model_class(random_state=current_seed, **model_params)\n",
    "                    model.fit(X_train, y_train)\n",
    "\n",
    "                    ### location-specific evaluation\n",
    "                    for location in locations:\n",
    "                        # Get test data for this location\n",
    "                        test_data_loc = trait_data[\n",
    "                            (trait_data['sample.id'].isin(test_genotypes)) & \n",
    "                            (trait_data['location'] == location)\n",
    "                        ].copy()\n",
    "                        \n",
    "                        if len(test_data_loc) < 3:\n",
    "                            continue\n",
    "\n",
    "                        # Make predictions\n",
    "                        X_test_loc = test_data_loc[feature_columns]\n",
    "                        y_pred = model.predict(X_test_loc)\n",
    "                        test_data_loc['predictions'] = y_pred\n",
    "                        \n",
    "                        # Average predictions and observations by genotype\n",
    "                        # this accounts for multiple observations per genotype within location\n",
    "                        genotype_averages = test_data_loc.groupby('sample.id').agg({\n",
    "                            trait: 'mean',\n",
    "                            'predictions': 'mean'\n",
    "                        }).reset_index()\n",
    "                        \n",
    "                        # Calculate evaluation metrics\n",
    "                        if len(genotype_averages) >= 3:\n",
    "                            y_true = genotype_averages[trait].values\n",
    "                            y_pred_vals = genotype_averages['predictions'].values\n",
    "                            \n",
    "                            # get Pearson and NDCG@10\n",
    "                            eval_results = evaluate_predictions(y_true, y_pred_vals, trait_name=trait)\n",
    "                            \n",
    "                            # Store results if valid\n",
    "                            if not np.isnan(eval_results['pearson']):\n",
    "                                new_row = pd.DataFrame({\n",
    "                                    'iteration': [iter_num],\n",
    "                                    'fold': [fold_idx],\n",
    "                                    'trait': [trait],\n",
    "                                    'location': [location],\n",
    "                                    'pearson': [eval_results['pearson']],\n",
    "                                    'ndcg_at_10': [eval_results['ndcg_at_10']],\n",
    "                                    'seed': [current_seed],\n",
    "                                    'n_test_genotypes': [len(genotype_averages)]\n",
    "                                })\n",
    "                                cv_results = pd.concat([cv_results, new_row], ignore_index=True)\n",
    "                                \n",
    "                                print(f\"    {trait} - {location} - Pearson: {eval_results['pearson']:.3f}, \"\n",
    "                                      f\"NDCG@10: {eval_results['ndcg_at_10']:.3f} | N: {len(genotype_averages)}\")\n",
    "                            else:\n",
    "                                print(f\"    {trait} - {location} - Invalid results\")\n",
    "                        else:\n",
    "                            print(f\"    {trait} - {location} - Insufficient data (N={len(genotype_averages)})\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"    Error in trait {trait}, iteration {iter_num}, fold {fold_idx}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    ### summarize results\n",
    "    if len(cv_results) > 0:\n",
    "        metric_cols = ['pearson', 'ndcg_at_10']\n",
    "        summary_stats = cv_results.groupby(['trait', 'location']).agg({\n",
    "            **{col: ['mean', 'std', 'min', 'max'] for col in metric_cols},\n",
    "            'n_test_genotypes': 'mean'\n",
    "        }).round(4)\n",
    "        \n",
    "        # Flatten column names\n",
    "        summary_stats.columns = [f\"{col}_{stat}\" if col in metric_cols else col \n",
    "                                for col, stat in summary_stats.columns]\n",
    "        summary_stats = summary_stats.reset_index()\n",
    "        \n",
    "        print(\"\\n=== Cross-Validation Summary by Location ===\")\n",
    "        print(summary_stats[['trait', 'location', 'pearson_mean', 'ndcg_at_10_mean']])\n",
    "\n",
    "        return cv_results, summary_stats\n",
    "    else:\n",
    "        print(\"No successful cross-validation results obtained\")\n",
    "        return pd.DataFrame(), pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e45182",
   "metadata": {},
   "source": [
    "## execute model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3713d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from file_with_functions import run_location_specific_cv   # import the function from .py file if above code is saved in a .py file\n",
    "import pandas as pd\n",
    "\n",
    "# Model parameters\n",
    "model_params = {\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 500\n",
    "}\n",
    "\n",
    "# Load model input data\n",
    "model_input = pd.read_pickle('../model_inputs/model_input.pkl')\n",
    "\n",
    "# Run cross-validation with specified traits\n",
    "results, summary = run_location_specific_cv(\n",
    "    data=model_input,\n",
    "    traits=['PtHt_mean', 'PcleLng_mean', 'SdW_z_mean', 'TGW_mean', 'SdLen_mean', 'DTF_mean', 'DTH_mean', 'PtHt_blue', 'PcleLng_blue', 'SdW_z_blue', 'TGW_blue', 'SdLen_blue', 'DTF_blue', 'DTH_blue'],\n",
    "    model_class=LGBMRegressor, \n",
    "    model_params=model_params,\n",
    "    n_iterations=15,\n",
    "    k_folds=5,\n",
    "    apply_zscore=True\n",
    ")\n",
    "\n",
    "# Save results \n",
    "results.to_pickle(f'../data/LightGBM_results.pkl')\n",
    "\n",
    "print(f\"\\nResults saved:\")\n",
    "print(f\"- Detailed results: ../data/LightGBM_results.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7819f760",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
